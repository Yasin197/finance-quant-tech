{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a52722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3e0133",
   "metadata": {},
   "source": [
    "Suppose one has data that consists of an independent vector and a dependent vector \n",
    " $x_i$ and \n",
    " (\n",
    " $i$ is the ith value in the data set). For example:\n",
    " -  $x_i$ is the height of the \n",
    "th person, and \n",
    " is their weight (predict weight using height)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7a104e",
   "metadata": {},
   "source": [
    "The goal of a neural network is as follows. Define a function $f$ that depends on parameters $`a`$ that makes predictions:\n",
    "$$\\hat{y_i} =f(x_i;a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bfbd63",
   "metadata": {},
   "source": [
    "One wants to make $\\hat{y_i}$ (the predictions) and $y_i$ (the true values) as `close as possible` by modifying the values of $a$. What does as close as possible mean? This depends on the task. In general, one defines a `similarity function` (or **Loss** function) $L(y,\\hat{y})$. The more similar all the $y_i$s and $\\hat{y_i}$s are, the smaller $L$ should be. For example 1 above, this could be as simple as:\n",
    "$$L(y,\\hat{y}) = \\sum_i(y_i-\\hat{y_i})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4642544a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# independent variable\n",
    "x = torch.tensor([[6,2],[5,2],[1,3],[7,6]]).float()\n",
    "\n",
    "# deperndant variable\n",
    "y = torch.tensor([[1,5,2,5]]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349ba278",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea13195",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bbca5d",
   "metadata": {},
   "source": [
    "* So $x_1 = (6,2)$, $x_2=(5,2)$, ...\n",
    "* So $y_1 = 1$, $y_2=5$, ...\n",
    "\n",
    "We want to find a function $f$ that depends on parameters $a$ that lets us get from $x$ to $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a06de22",
   "metadata": {},
   "source": [
    "**Idea**:\n",
    "1. First multiply each element in $x$ by a $8 \\times 2$ matrix (this is 16 parameters $a_i$) - first layer\n",
    "2. Then multiply each element in $x$ by a $1 \\times 8$ matrix (this is 8 parameters $a_i$) - second layer\n",
    "\n",
    "Define a matrix (takes in a 2d vector and returns a 8d vector)\n",
    "\n",
    "- IMPORTANT: When the matrix is created, it is initially created with random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c9361e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so takes in 2 different features (as above e.g. x1 = (6, 2)) and turns that into 8 features\n",
    "M1 = nn.Linear(2,8,bias=False)\n",
    "M1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d428d5",
   "metadata": {},
   "source": [
    "If one passes in a vector $x$\n",
    " (the dataset) where each element $x$ \n",
    " (an instance) is a 2d vector, $M$\n",
    " will apply the same matrix multiplication to each element $xi$\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300c3a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now you can see the 4 independent varibles which was a 2d vector is now an 8d vector\n",
    "M1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d287bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes in an 8d vecotr as above and provides a 1d vector\n",
    "M2 = nn.Linear(8,1,bias=False)\n",
    "M2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ce2410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we chain them together\n",
    "M2(M1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8b556a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can see this is 2 dimensional where one of them is 1 but we will need to match is to y\n",
    "M2(M1(x)).shape , y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba060ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we can call the squeeze method on the layers to make the same shape as y\n",
    "M2(M1(x)).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9fbdbe",
   "metadata": {},
   "source": [
    "The weights of the matrices `M1` and `M2` consitute the weights $a$ of the network defined above. In order to optimize for these weights, we first construct our network $f$ as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bdb34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Module would typically be used as a base for a super class as there is a lot of functionality - but as a user we just need to define a few things for ourselves\n",
    "class MyNeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.Matrix1 = nn.Linear(2,8,bias=False)\n",
    "        self.Matrix2 = nn.Linear(8,1,bias=False)\n",
    "    def forward(self,x):\n",
    "        x = self.Matrix1(x)\n",
    "        x = self.Matrix2(x)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b1ff3d",
   "metadata": {},
   "source": [
    "Constructing the network using a subclass of the `nn.Module` allows the parameters of the network to be conveniently stored. This will be useful later when we need to adjust them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e79856",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = MyNeuralNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c477b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass in data to the network.\n",
    "yhat = f(x)\n",
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e74ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1909ed59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this class there is a method called parameters and i can loop through them and call it like so\n",
    "for par in f.parameters():\n",
    "    print(par)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5058306a",
   "metadata": {},
   "source": [
    "# Adjusting $a$ so that $\\hat{y}$ and $y$ are similar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2adadc",
   "metadata": {},
   "source": [
    "Now we define the loss function $L$, which provides a metric of similarity between $y$ and $\\hat{y}$. In this case, we will use the `mean squared error loss` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef79b3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = nn.MSELoss()\n",
    "L(y,yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4527a88d",
   "metadata": {},
   "source": [
    "Confirming it is doing the same as the regular mean-squared error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334e61b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean((y-yhat)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab93d90f",
   "metadata": {},
   "source": [
    "Note that $L$ depends on $a$, since our predictions $\\hat{y}$ depend on the parameters of the network $a$. In this sense, $L=L(a)$. **The main idea behind machine learning** is to compute the gradient or derivative with respect to each parameter\n",
    "$$\\frac{\\partial L}{\\partial a_i}$$\n",
    "for each parameter $a_i$ of the network. Then we adjust each parameter as follows:\n",
    "$$a_i \\to a_i - \\ell \\frac{\\partial L}{\\partial a_i}$$\n",
    "where $\\ell$ is the learning rate.\n",
    "\n",
    "**Example**: A loss function that only depends on one parameter:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7655d7",
   "metadata": {},
   "source": [
    "The idea is to do this over and over again, until one reaches a minimum for $L$. This is called **gradient descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cec4a2",
   "metadata": {},
   "source": [
    "* Each pass of the full data set $x$ is called an **epoch**. In this case, we are evaluating $\\partial L/\\partial a_i$ on the entire dataset $x$ each time we iterate $a_i \\to a_i - \\ell \\frac{\\partial L}{\\partial a_i}$, so each iteration corresponds to an epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cda5727",
   "metadata": {},
   "source": [
    "The `SGD`(stochastic gradient descent) takes in all model parameters $a$ along with the learning rate $\\ell$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9d238b",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = SGD(f.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e380832",
   "metadata": {},
   "source": [
    "Adjust the parameters over and over:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20f0c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for _ in range(50):\n",
    "    opt.zero_grad() # flush previous epoch's gradient\n",
    "    loss_value = L(f(x), y) #compute loss - remember f is the model and x is the data\n",
    "    loss_value.backward() # compute gradient\n",
    "    opt.step() # Perform iteration using gradient above - this just adjusts all the parameters\n",
    "    losses.append(loss_value.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f6aebe",
   "metadata": {},
   "source": [
    "Plot $L(a)$ as a function of number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81770f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses);\n",
    "plt.ylabel('Loss $L(y,\\hat{y};a)$');\n",
    "plt.xlabel('Epochs');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4772cc",
   "metadata": {},
   "source": [
    "This is as close as we can make the model $f$ predict $y$ from $x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ad199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9bcdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa2751c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
