{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b08dbdf",
   "metadata": {},
   "source": [
    "## PyTorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5067644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# https://docs.astral.sh/uv/guides/integration/pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6ef8e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.   0.25 0.5  0.75 1.  ] tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "# mumpy arrays and torch tensors can be created in the same way\n",
    "n = np.linspace(0,1,5)\n",
    "t = torch.linspace(0,1,5)\n",
    "print(n, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a125fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0  1  2  3]\n",
      "  [ 4  5  6  7]\n",
      "  [ 8  9 10 11]\n",
      "  [12 13 14 15]]\n",
      "\n",
      " [[16 17 18 19]\n",
      "  [20 21 22 23]\n",
      "  [24 25 26 27]\n",
      "  [28 29 30 31]]\n",
      "\n",
      " [[32 33 34 35]\n",
      "  [36 37 38 39]\n",
      "  [40 41 42 43]\n",
      "  [44 45 46 47]]]\n"
     ]
    }
   ],
   "source": [
    "# they can be resized in similar ways - so 48 elemnts reshaped into a multi dimensional array of 3x4x4\n",
    "n = np.arange(48).reshape(3,4,4)\n",
    "t = torch.arange(48).reshape(3,4,4)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ed1708",
   "metadata": {},
   "source": [
    "### Broadcasting rules\n",
    "\n",
    "when operating on two arrays, NumPy comapres their shapes element-wise, starts with trailing i.e. rightmost dimensions and works its way left. Two dimensions are compatible when:\n",
    "- they are **equal**, or\n",
    "- one of them is **1**\n",
    "\n",
    "**Example:**\n",
    "<br>\n",
    "Shape 1: (1,6,4,1,7,2)\n",
    "<br>\n",
    "Shape 2: (5,6,1,3,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82a5025a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2])\n",
    "b = np.array([3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d586cca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 8])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e214f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 2\n",
    "a = np.ones((6,5))\n",
    "b = np.arange(5).reshape((1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc07a2a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3eeaf1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 3, 4]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "758407e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d65bc9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d03a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3., 4., 5.],\n",
       "       [1., 2., 3., 4., 5.],\n",
       "       [1., 2., 3., 4., 5.],\n",
       "       [1., 2., 3., 4., 5.],\n",
       "       [1., 2., 3., 4., 5.],\n",
       "       [1., 2., 3., 4., 5.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can see as b is 1 dimensional it just gets duplicated by the rows\n",
    "a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d68c3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones((6,5))\n",
    "b = torch.arange(5).reshape((1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a45a7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3., 4., 5.],\n",
       "        [1., 2., 3., 4., 5.],\n",
       "        [1., 2., 3., 4., 5.],\n",
       "        [1., 2., 3., 4., 5.],\n",
       "        [1., 2., 3., 4., 5.],\n",
       "        [1., 2., 3., 4., 5.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a+b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1efb0e",
   "metadata": {},
   "source": [
    "The arrays/tensors don't need to have the same number of dimenions. If one of the arrays/tensors has less dimensions than the other\n",
    "\n",
    "**Example:** Scaling each other the color channels of an image by a different amount:\n",
    "\n",
    "Image  (3d array): 256 x 256 x 3\n",
    "<br>\n",
    "Scale  (1d array): &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;3\n",
    "<br>\n",
    "Result (3d array): 256 x 256 x 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5322ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image = torch.randn((256,256,3))\n",
    "Scale = torch.tensor([0.5,1.5,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66d0dae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0969,  1.0488,  0.0730],\n",
       "         [-0.5482,  0.7016,  0.4396],\n",
       "         [ 0.3996,  0.9785,  0.2901],\n",
       "         ...,\n",
       "         [-0.4893, -0.1830,  0.1362],\n",
       "         [-0.8940, -0.1338,  1.5722],\n",
       "         [ 0.3702, -0.4926, -0.6675]],\n",
       "\n",
       "        [[ 0.9726, -1.5398, -0.6690],\n",
       "         [ 0.7559,  0.0918,  0.2200],\n",
       "         [-0.5929, -0.3992,  1.3168],\n",
       "         ...,\n",
       "         [-1.8319, -1.5093,  1.1236],\n",
       "         [-0.1744,  1.3737,  0.9968],\n",
       "         [-0.0169,  0.6150, -0.1633]],\n",
       "\n",
       "        [[-0.6489,  0.6072,  0.2009],\n",
       "         [-0.2373,  0.3927, -2.8041],\n",
       "         [ 0.5025,  0.0210, -1.1520],\n",
       "         ...,\n",
       "         [ 0.8752, -0.2298,  0.2984],\n",
       "         [ 0.3933,  0.3698, -0.5021],\n",
       "         [ 0.4035, -1.0096, -1.1452]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.6455, -0.5438, -0.0592],\n",
       "         [-0.1962,  0.6288,  0.7587],\n",
       "         [-1.6470,  0.9418, -0.8703],\n",
       "         ...,\n",
       "         [-0.6489,  0.5952,  0.3803],\n",
       "         [-0.9080,  1.8951,  0.1763],\n",
       "         [-1.2206,  0.7156, -1.0505]],\n",
       "\n",
       "        [[ 0.6446, -1.0774,  1.6049],\n",
       "         [ 1.1120,  0.8406, -0.4084],\n",
       "         [ 0.3562, -0.2002,  2.6806],\n",
       "         ...,\n",
       "         [ 0.6863, -0.3121, -0.6353],\n",
       "         [ 0.3076, -0.0792, -2.2158],\n",
       "         [-0.2030, -0.8301, -0.1111]],\n",
       "\n",
       "        [[-0.7110, -1.1632, -0.0378],\n",
       "         [-0.5823, -0.2749, -1.2936],\n",
       "         [-0.1912,  2.3236, -2.7623],\n",
       "         ...,\n",
       "         [-0.8832, -1.6730, -1.3347],\n",
       "         [-1.2390, -0.5915,  1.7993],\n",
       "         [-0.3637, -2.0481,  0.3781]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8acbbbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Result = Image*Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4626e9f6",
   "metadata": {},
   "source": [
    "**Example:** One has an array of 2 images and wants to scale the color channels of each image by a slightly different amount:\n",
    "\n",
    "&nbsp;Images  (4d array): 2 x 256 x 256 x 3\n",
    "<br>\n",
    "&nbsp;Scales  (4d array): 2 x 1 x 1 x 3\n",
    "<br>\n",
    "&nbsp;Results  (4d array): 2 x 256 x 256 x 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f003f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "Images = torch.randn((2,256,256,3))\n",
    "Scales = torch.tensor([0.5,1.5,1,1.5,1,0.5]).reshape((2,1,1,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "07f25b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Result = Image*Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0492ed16",
   "metadata": {},
   "source": [
    "## Operations Across Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd8a3ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.1250), tensor(1.6520), tensor(4.), tensor(0.5000))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([0.5,1,3,4])\n",
    "torch.mean(t), torch.std(t), torch.max(t), torch.min(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4458b0",
   "metadata": {},
   "source": [
    "But suppose we have a 2d tensor, for example, and want to compute the mean value of each columns:\n",
    "\n",
    "- Note: taking the mean of each column means taking the mean across the rows (which are the first dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62ca1e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.arange(20, dtype=float).reshape(5,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91d9173a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.],\n",
       "        [12., 13., 14., 15.],\n",
       "        [16., 17., 18., 19.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9abf32c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.], dtype=torch.float64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remember rows are the first dimension in 2d arrays\n",
    "t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1a5eac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8.,  9., 10., 11.], dtype=torch.float64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(t, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b608a1ca",
   "metadata": {},
   "source": [
    "This can be done for higher dimensionality arrays as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5987f84f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 256, 3])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.randn(5,256,256,3)\n",
    "# Take the mean across the batch (size 5)\n",
    "torch.mean(t,axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "34bee651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 256, 256])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take the mean across the color channels which is the last axis\n",
    "torch.mean(t,axis=-1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aeafdd",
   "metadata": {},
   "source": [
    "Take only the maximum color channel values (and get the corresponding indices):\n",
    "- This is done all the time in image segmentation models (i.e. take an image, decide which pixels correspond to, say, a car)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab5585cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "values, indices = torch.max(t,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4cad1482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 256, 256])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "766ee412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 0, 2,  ..., 0, 1, 2],\n",
       "         [2, 0, 0,  ..., 2, 0, 0],\n",
       "         [1, 0, 2,  ..., 0, 2, 2],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 1, 1],\n",
       "         [2, 2, 2,  ..., 1, 1, 1],\n",
       "         [0, 2, 0,  ..., 0, 1, 1]],\n",
       "\n",
       "        [[0, 1, 2,  ..., 0, 1, 2],\n",
       "         [2, 1, 1,  ..., 2, 0, 2],\n",
       "         [2, 0, 1,  ..., 2, 0, 1],\n",
       "         ...,\n",
       "         [1, 1, 2,  ..., 2, 0, 2],\n",
       "         [2, 2, 2,  ..., 0, 0, 1],\n",
       "         [1, 1, 2,  ..., 0, 2, 2]],\n",
       "\n",
       "        [[2, 0, 0,  ..., 0, 0, 1],\n",
       "         [2, 0, 2,  ..., 2, 0, 2],\n",
       "         [0, 0, 1,  ..., 2, 1, 2],\n",
       "         ...,\n",
       "         [0, 2, 2,  ..., 0, 1, 2],\n",
       "         [0, 0, 1,  ..., 2, 2, 0],\n",
       "         [1, 2, 0,  ..., 0, 1, 1]],\n",
       "\n",
       "        [[2, 2, 0,  ..., 2, 0, 1],\n",
       "         [2, 1, 0,  ..., 2, 1, 1],\n",
       "         [0, 1, 2,  ..., 1, 2, 1],\n",
       "         ...,\n",
       "         [0, 1, 2,  ..., 2, 1, 2],\n",
       "         [2, 2, 0,  ..., 1, 1, 1],\n",
       "         [0, 1, 0,  ..., 1, 2, 0]],\n",
       "\n",
       "        [[0, 1, 1,  ..., 0, 2, 1],\n",
       "         [0, 1, 0,  ..., 1, 2, 2],\n",
       "         [2, 1, 1,  ..., 1, 1, 2],\n",
       "         ...,\n",
       "         [1, 0, 2,  ..., 0, 2, 1],\n",
       "         [0, 1, 1,  ..., 0, 1, 1],\n",
       "         [0, 2, 1,  ..., 2, 2, 2]]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04867024",
   "metadata": {},
   "source": [
    "### **Pytorch** starts to really differ from **numpy** in terms of automatically computing gradients of operations\n",
    "$$y = \\sum_i x_i^3 has a gradient \\frac{\\partial y}{\\partial x_i} = 3x_i^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ee8590f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[5.,8.],[4.,6.]], requires_grad=True)\n",
    "y = x.pow(3).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4da8e177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[5., 8.],\n",
       "         [4., 6.]], requires_grad=True),\n",
       " tensor(917., grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0607da67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 75., 192.],\n",
       "        [ 48., 108.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward() #compute the gradient\n",
    "x.grad #print the gradient (everything that has happened to x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c50730",
   "metadata": {},
   "source": [
    "Double check using the analytical formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "da5e23aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 75., 192.],\n",
       "        [ 48., 108.]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3*x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8994647",
   "metadata": {},
   "source": [
    " In the context of machine learning, X\n",
    " contains all the weights (also known as parameters) of the neural network and \n",
    " is the Loss Function of the neural network. Thats what gradients tell us, if we change something, how does the output change.\n",
    "\n",
    " A torch array will give you faster operations but is a more heavy object than for a numpy array and uses up more memory which is the tradeoff"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
